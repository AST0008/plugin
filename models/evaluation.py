# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_Y34CZicZlTt_NEQuyejJdsQugGTNabJ
"""

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

def evaluate_transcription(transcription):
    """
    Evaluates a given transcription using the specified model and tokenizer.

    Args:
        transcription (str): Text from the STT function.

    Returns:
        str: Model-generated feedback report.
    """
    # Hugging Face model details
    hf_token = "your_hf_token"  # Replace with your Hugging Face token
    model_name = "Telugu-LLM-Labs/Indic-gemma-2b-finetuned-sft-Navarasa-2.0"

    # Load the model
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        load_in_4bit=False,
        token=hf_token
    )
    model.to("cuda")

    # Load the tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    # Define the input prompt
    input_prompt = """
    ### Instruction:
    Prompt for Generating a Communication Assessment Report
    
    Evaluate the user's speaking performance and generate a Communication Assessment Report with the following sections:
    
    Rating (Out of 10):
    Provide a score (0–10) based on grammar, pronunciation, clarity, and fluency, with a brief explanation.
    
    Corrections/What Went Wrong:
    List specific errors, including:
    - Grammar mistakes (provide examples and corrections).
    - Overuse of filler words (e.g., "um," "uh," "like").
    - Unnatural pauses or hesitations.
    
    Things That Could Have Been Better:
    Suggest areas for refinement, such as sentence variety, vocabulary usage, or overall engagement.
    
    Suggestions:
    Recommend tailored improvements, including:
    - Practice exercises (e.g., reading aloud, tongue twisters).
    - Learning resources (e.g., apps for grammar or pronunciation).
    - Interactive activities (e.g., speaking clubs, self-review).
    - Expert guidance (e.g., tutors, workshops).
    
    Keep the feedback clear, actionable, and user-focused.
    
    ### Input:
    {}

    ### Response:
    {}"""

    # Format the input prompt with the evaluation criteria
    formatted_input = input_prompt.format(
        transcription,
        ""  # Leave this blank for generation
    )

    # Tokenize and move inputs to GPU
    inputs = tokenizer([formatted_input], return_tensors="pt").to("cuda")

    # Generate the model response
    outputs = model.generate(**inputs, max_new_tokens=300, use_cache=True)

    # Decode the response
    decoded_output = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]

    # Extract the feedback part (after "### Response:")
    feedback = decoded_output.split("### Response:")[-1].strip()

    return feedback

# Example transcription from STT function
stt_transcription = "बाद ही अब जाऊंगी भाई नहीं मेरे को दर्शिका नहीं उसने बोला नहीं दर्शिकाझेल भाई और बाकी मतलब ऐसा हुआ था कि म कल क्या हुआ था कल रात में मैंने लीके इसके ह वाली एक रूमट रूमेट है शालेनी है उसको मैंने बोल दिया था कि भाई रूम लॉक ना करना पर दर्जगा नहीं थी उससे म रूम पे इसलिए मै उसको नहीं बोल पाई थी तो मतलब चार बजे आए थे लिटर दर्जगा के खोलने आई है अंदर् जी ने एक बार भी कोई एक अंदा मुँह नहीं बनाया एक बार भी कुछ बोला नहीं सलीटिल सॉिया मेरे को नहीं पता मतलब मैं शॉालिनी से बोली थी यार मतलब मैंने बहुत सारी सारी बोला म शिव चिल काफी चलती हुई वहा"

# Evaluate the transcription and generate feedback
feedback_report = evaluate_transcription(stt_transcription)

# Display the feedback report
print("Feedback Report:\n", feedback_report)
